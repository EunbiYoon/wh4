[
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "forward_propagation",
        "importPath": "propagation",
        "description": "propagation",
        "isExtraImport": true,
        "detail": "propagation",
        "documentation": {}
    },
    {
        "label": "cost_function",
        "importPath": "propagation",
        "description": "propagation",
        "isExtraImport": true,
        "detail": "propagation",
        "documentation": {}
    },
    {
        "label": "backpropagation_vectorized",
        "importPath": "propagation",
        "description": "propagation",
        "isExtraImport": true,
        "detail": "propagation",
        "documentation": {}
    },
    {
        "label": "forward_propagation",
        "importPath": "propagation",
        "description": "propagation",
        "isExtraImport": true,
        "detail": "propagation",
        "documentation": {}
    },
    {
        "label": "cost_function",
        "importPath": "propagation",
        "description": "propagation",
        "isExtraImport": true,
        "detail": "propagation",
        "documentation": {}
    },
    {
        "label": "run_debug",
        "importPath": "propagation",
        "description": "propagation",
        "isExtraImport": true,
        "detail": "propagation",
        "documentation": {}
    },
    {
        "label": "NeuralNetwork",
        "importPath": "nn",
        "description": "nn",
        "isExtraImport": true,
        "detail": "nn",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "nn",
        "description": "nn",
        "isExtraImport": true,
        "detail": "nn",
        "documentation": {}
    },
    {
        "label": "stratified_k_fold_split",
        "importPath": "nn",
        "description": "nn",
        "isExtraImport": true,
        "detail": "nn",
        "documentation": {}
    },
    {
        "label": "my_accuracy",
        "importPath": "nn",
        "description": "nn",
        "isExtraImport": true,
        "detail": "nn",
        "documentation": {}
    },
    {
        "label": "my_f1_score",
        "importPath": "nn",
        "description": "nn",
        "isExtraImport": true,
        "detail": "nn",
        "documentation": {}
    },
    {
        "label": "save_metrics_table",
        "importPath": "nn",
        "description": "nn",
        "isExtraImport": true,
        "detail": "nn",
        "documentation": {}
    },
    {
        "label": "plot_best_learning_curve",
        "importPath": "nn",
        "description": "nn",
        "isExtraImport": true,
        "detail": "nn",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "OneHotEncoder",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "debug_text",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "debug_text",
        "description": "debug_text",
        "detail": "debug_text",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "debug_text",
        "description": "debug_text",
        "peekOfCode": "def main(lambda_reg, X, y, Theta, all_a_lists, all_z_lists, J_list, final_cost, delta_list, D_list, finalized_D): \n    with open(\"debug/backprop_debug.txt\", \"w\", encoding=\"utf-8\") as f:\n        np.set_printoptions(precision=5, suppress=True, floatmode='fixed')\n        def println(line=\"\"):\n            f.write(line + \"\\n\")\n        println(f\"Regularization parameter lambda={lambda_reg:.3f}\\n\")\n        structure = [len(X[0])] + [len(t) for t in Theta]\n        println(\"Initializing the network with the following structure (number of neurons per layer): \" + str(structure).replace(\",\", \"\") + \"\\n\")\n        for i, theta in enumerate(Theta):\n            println(f\"Initial Theta{i+1} (the weights of each neuron, including the bias weight, are stored in the rows):\")",
        "detail": "debug_text",
        "documentation": {}
    },
    {
        "label": "flatten_weights",
        "kind": 2,
        "importPath": "extra3",
        "description": "extra3",
        "peekOfCode": "def flatten_weights(weights):\n    return np.concatenate([w.flatten() for w in weights])\ndef unflatten_weights(flattened, shapes):\n    weights = []\n    idx = 0\n    for shape in shapes:\n        size = np.prod(shape)\n        weights.append(flattened[idx:idx + size].reshape(shape))\n        idx += size\n    return weights",
        "detail": "extra3",
        "documentation": {}
    },
    {
        "label": "unflatten_weights",
        "kind": 2,
        "importPath": "extra3",
        "description": "extra3",
        "peekOfCode": "def unflatten_weights(flattened, shapes):\n    weights = []\n    idx = 0\n    for shape in shapes:\n        size = np.prod(shape)\n        weights.append(flattened[idx:idx + size].reshape(shape))\n        idx += size\n    return weights\n# === Cost Function Wrapper ===\ndef make_cost_function(X, y, shapes, lambda_reg):",
        "detail": "extra3",
        "documentation": {}
    },
    {
        "label": "make_cost_function",
        "kind": 2,
        "importPath": "extra3",
        "description": "extra3",
        "peekOfCode": "def make_cost_function(X, y, shapes, lambda_reg):\n    def J(theta_flat):\n        weights = unflatten_weights(theta_flat, shapes)\n        all_a_lists, _ = forward_propagation(weights, X)\n        pred_y_list = [a_list[-1] for a_list in all_a_lists]\n        _, final_cost = cost_function(pred_y_list, y, weights, lambda_reg)\n        return final_cost\n    return J\n# === Numerical Gradient Computation ===\ndef compute_numerical_gradient(J, theta, epsilon=1e-4):",
        "detail": "extra3",
        "documentation": {}
    },
    {
        "label": "compute_numerical_gradient",
        "kind": 2,
        "importPath": "extra3",
        "description": "extra3",
        "peekOfCode": "def compute_numerical_gradient(J, theta, epsilon=1e-4):\n    num_grad = np.zeros_like(theta)\n    for i in range(len(theta)):\n        theta_plus = np.copy(theta)\n        theta_minus = np.copy(theta)\n        theta_plus[i] += epsilon\n        theta_minus[i] -= epsilon\n        num_grad[i] = (J(theta_plus) - J(theta_minus)) / (2 * epsilon)\n    return num_grad\n# === Main Entry for Numerical Gradient Update ===",
        "detail": "extra3",
        "documentation": {}
    },
    {
        "label": "numerical_gradient_update",
        "kind": 2,
        "importPath": "extra3",
        "description": "extra3",
        "peekOfCode": "def numerical_gradient_update(weights, X, y, lambda_reg, alpha):\n    shapes = [w.shape for w in weights]\n    theta_flat = flatten_weights(weights)\n    J = make_cost_function(X, y, shapes, lambda_reg)\n    grad_flat = compute_numerical_gradient(J, theta_flat)\n    grad_structured = unflatten_weights(grad_flat, shapes)\n    # Update weights manually here and return new ones\n    updated_weights = [w - alpha * g for w, g in zip(weights, grad_structured)]\n    return updated_weights\ndef main():",
        "detail": "extra3",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "extra3",
        "description": "extra3",
        "peekOfCode": "def main():\n    X, y = load_dataset()\n    folds = stratified_k_fold_split(X, y, k=5)\n    lambda_reg_list = [0.1, 0.01]  # Regularization values\n    hidden_layers = [[32],[32,16],[32,16,8]]  # Different architectures\n    alpha = 0.01\n    epochs = 100\n    batch_size = 16\n    mode = \"mini-batch\"\n    dataset_name = DATASET_NAME",
        "detail": "extra3",
        "documentation": {}
    },
    {
        "label": "DATASET_NAME",
        "kind": 5,
        "importPath": "extra3",
        "description": "extra3",
        "peekOfCode": "DATASET_NAME = \"wdbc\"\n# === Flatten and Unflatten Functions ===\ndef flatten_weights(weights):\n    return np.concatenate([w.flatten() for w in weights])\ndef unflatten_weights(flattened, shapes):\n    weights = []\n    idx = 0\n    for shape in shapes:\n        size = np.prod(shape)\n        weights.append(flattened[idx:idx + size].reshape(shape))",
        "detail": "extra3",
        "documentation": {}
    },
    {
        "label": "NeuralNetwork",
        "kind": 6,
        "importPath": "nn",
        "description": "nn",
        "peekOfCode": "class NeuralNetwork:\n    def __init__(self, layer_sizes, alpha=0.01, lambda_reg=0.0):\n        self.layer_sizes = layer_sizes\n        self.alpha = alpha\n        self.lambda_reg = lambda_reg\n        self.weights = self.initialize_weights()\n        self.cost_history = []\n    def initialize_weights(self):\n        weights = []\n        for i in range(len(self.layer_sizes) - 1):",
        "detail": "nn",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "kind": 2,
        "importPath": "nn",
        "description": "nn",
        "peekOfCode": "def load_dataset():\n    DATA_PATH = f\"datasets/{DATASET_NAME}.csv\"\n    df = pd.read_csv(DATA_PATH)\n    if 'label' not in df.columns:\n        raise ValueError(\"Dataset must contain a 'label' column.\")\n    y = df['label'].copy()\n    X = df.drop(columns=['label'])\n    # Normalize numeric columns and apply one-hot encoding to categorical columns\n    for col in X.columns:\n        if col.endswith(\"_num\"):",
        "detail": "nn",
        "documentation": {}
    },
    {
        "label": "stratified_k_fold_split",
        "kind": 2,
        "importPath": "nn",
        "description": "nn",
        "peekOfCode": "def stratified_k_fold_split(X, y, k=5):\n    df = pd.DataFrame(X)\n    df['label'] = y.ravel()\n    class_0 = df[df['label'] == 0].sample(frac=1).reset_index(drop=True)\n    class_1 = df[df['label'] == 1].sample(frac=1).reset_index(drop=True)\n    folds = []\n    for i in range(k):\n        # test data\n        c0 = class_0.iloc[int(len(class_0)*i/k):int(len(class_0)*(i+1)/k)]\n        c1 = class_1.iloc[int(len(class_1)*i/k):int(len(class_1)*(i+1)/k)]",
        "detail": "nn",
        "documentation": {}
    },
    {
        "label": "my_accuracy",
        "kind": 2,
        "importPath": "nn",
        "description": "nn",
        "peekOfCode": "def my_accuracy(y_true, y_pred):\n    correct = np.sum(y_true == y_pred)\n    return correct / len(y_true)\n# === F1 Score Calculation ===\ndef my_f1_score(y_true, y_pred):\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    if tp == 0:\n        return 0.0",
        "detail": "nn",
        "documentation": {}
    },
    {
        "label": "my_f1_score",
        "kind": 2,
        "importPath": "nn",
        "description": "nn",
        "peekOfCode": "def my_f1_score(y_true, y_pred):\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    if tp == 0:\n        return 0.0\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n    if precision + recall == 0:\n        return 0.0",
        "detail": "nn",
        "documentation": {}
    },
    {
        "label": "plot_best_learning_curve",
        "kind": 2,
        "importPath": "nn",
        "description": "nn",
        "peekOfCode": "def plot_best_learning_curve(results, dataset_name, save_folder):\n    best_key = min(results, key=lambda k: results[k]['model'].cost_history[-1])\n    best_info = results[best_key]\n    model = best_info['model']\n    hidden_layer = best_info['hidden']\n    lambda_reg = best_info['lambda_reg']\n    train_size = best_info['train_size']\n    alpha = best_info['alpha']\n    mode = best_info['mode']\n    batch_size = best_info['batch_size']",
        "detail": "nn",
        "documentation": {}
    },
    {
        "label": "save_metrics_table",
        "kind": 2,
        "importPath": "nn",
        "description": "nn",
        "peekOfCode": "def save_metrics_table(results_by_dataset, save_folder):\n    os.makedirs(\"evaluation\", exist_ok=True)\n    for dataset_name, dataset_results in results_by_dataset.items():\n        fig, ax = plt.subplots()\n        ax.axis('off')\n        # ✅ 조건부로 Batch Size 컬럼 추가\n        if any(val['mode'] == 'mini-batch' for val in dataset_results.values()):\n            col_labels = [\"Layer & Neuron\", \"Lambda\", \"Alpha\", \"Batch Size\", \"Mode\", \"Avg Accuracy\", \"Avg F1 Score\"]\n            show_batch_size = True\n        else:",
        "detail": "nn",
        "documentation": {}
    },
    {
        "label": "neural_network",
        "kind": 2,
        "importPath": "nn",
        "description": "nn",
        "peekOfCode": "def neural_network():\n    X, y = load_dataset()\n    folds = stratified_k_fold_split(X, y, k=5)\n    lambda_reg_list = [0.001, 0.0001, 0.00001, 0.000001]  # Regularization values\n    hidden_layers = [[32],[32,16],[32,16,8]]  # Different architectures\n    alpha = 0.1\n    batch_size = 32\n    mode = \"batch\"\n    dataset_name = DATASET_NAME\n    results = {}",
        "detail": "nn",
        "documentation": {}
    },
    {
        "label": "DATASET_NAME",
        "kind": 5,
        "importPath": "nn",
        "description": "nn",
        "peekOfCode": "DATASET_NAME = \"raisin\"\nM_SIZE = 10\nDEBUG_MODE = True\n# === Neural Network Class ===\nclass NeuralNetwork:\n    def __init__(self, layer_sizes, alpha=0.01, lambda_reg=0.0):\n        self.layer_sizes = layer_sizes\n        self.alpha = alpha\n        self.lambda_reg = lambda_reg\n        self.weights = self.initialize_weights()",
        "detail": "nn",
        "documentation": {}
    },
    {
        "label": "M_SIZE",
        "kind": 5,
        "importPath": "nn",
        "description": "nn",
        "peekOfCode": "M_SIZE = 10\nDEBUG_MODE = True\n# === Neural Network Class ===\nclass NeuralNetwork:\n    def __init__(self, layer_sizes, alpha=0.01, lambda_reg=0.0):\n        self.layer_sizes = layer_sizes\n        self.alpha = alpha\n        self.lambda_reg = lambda_reg\n        self.weights = self.initialize_weights()\n        self.cost_history = []",
        "detail": "nn",
        "documentation": {}
    },
    {
        "label": "DEBUG_MODE",
        "kind": 5,
        "importPath": "nn",
        "description": "nn",
        "peekOfCode": "DEBUG_MODE = True\n# === Neural Network Class ===\nclass NeuralNetwork:\n    def __init__(self, layer_sizes, alpha=0.01, lambda_reg=0.0):\n        self.layer_sizes = layer_sizes\n        self.alpha = alpha\n        self.lambda_reg = lambda_reg\n        self.weights = self.initialize_weights()\n        self.cost_history = []\n    def initialize_weights(self):",
        "detail": "nn",
        "documentation": {}
    },
    {
        "label": "sigmoid",
        "kind": 2,
        "importPath": "propagation",
        "description": "propagation",
        "peekOfCode": "def sigmoid(z):\n    return 1 / (1 + np.exp(-z))\ndef sigmoid_gradient(z):\n    return sigmoid(z) * (1 - sigmoid(z))\ndef add_bias(X):\n    return np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\ndef forward_propagation(Theta, X):\n    Theta = [np.array(t) for t in Theta]\n    A = [add_bias(np.array(X))] \n    Z = []",
        "detail": "propagation",
        "documentation": {}
    },
    {
        "label": "sigmoid_gradient",
        "kind": 2,
        "importPath": "propagation",
        "description": "propagation",
        "peekOfCode": "def sigmoid_gradient(z):\n    return sigmoid(z) * (1 - sigmoid(z))\ndef add_bias(X):\n    return np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\ndef forward_propagation(Theta, X):\n    Theta = [np.array(t) for t in Theta]\n    A = [add_bias(np.array(X))] \n    Z = []\n    for i, Theta_i in enumerate(Theta):\n        Z_i = A[-1] @ Theta_i.T",
        "detail": "propagation",
        "documentation": {}
    },
    {
        "label": "add_bias",
        "kind": 2,
        "importPath": "propagation",
        "description": "propagation",
        "peekOfCode": "def add_bias(X):\n    return np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\ndef forward_propagation(Theta, X):\n    Theta = [np.array(t) for t in Theta]\n    A = [add_bias(np.array(X))] \n    Z = []\n    for i, Theta_i in enumerate(Theta):\n        Z_i = A[-1] @ Theta_i.T\n        A_i = sigmoid(Z_i)\n        Z.append(Z_i)",
        "detail": "propagation",
        "documentation": {}
    },
    {
        "label": "forward_propagation",
        "kind": 2,
        "importPath": "propagation",
        "description": "propagation",
        "peekOfCode": "def forward_propagation(Theta, X):\n    Theta = [np.array(t) for t in Theta]\n    A = [add_bias(np.array(X))] \n    Z = []\n    for i, Theta_i in enumerate(Theta):\n        Z_i = A[-1] @ Theta_i.T\n        A_i = sigmoid(Z_i)\n        Z.append(Z_i)\n        if i < len(Theta) - 1:\n            A_i = add_bias(A_i) # add bias in hidden layer only",
        "detail": "propagation",
        "documentation": {}
    },
    {
        "label": "cost_function",
        "kind": 2,
        "importPath": "propagation",
        "description": "propagation",
        "peekOfCode": "def cost_function(A_final, Y, Theta, lambda_reg):\n    m = Y.shape[0]\n    cost = -np.sum(Y * np.log(A_final) + (1 - Y) * np.log(1 - A_final)) / m\n    reg_term = 0\n    for theta in Theta:\n        theta = np.array(theta)\n        # remove bias -> [:, 1:]\n        reg_term += np.sum(theta[:, 1:] ** 2) \n    reg_term = reg_term * (lambda_reg / (2 * m))\n    return cost, cost + reg_term",
        "detail": "propagation",
        "documentation": {}
    },
    {
        "label": "backpropagation_vectorized",
        "kind": 2,
        "importPath": "propagation",
        "description": "propagation",
        "peekOfCode": "def backpropagation_vectorized(Theta, A, Z, Y, lambda_reg):\n    Theta = [np.array(t) for t in Theta]\n    m = Y.shape[0]\n    delta = A[-1] - Y\n    gradients = [None] * len(Theta)\n    for i in reversed(range(len(Theta))):\n        a_prev = A[i]\n        gradients[i] = (delta.T @ a_prev) / m\n        if i > 0:\n            delta = (delta @ Theta[i][:, 1:]) * sigmoid_gradient(Z[i - 1])",
        "detail": "propagation",
        "documentation": {}
    },
    {
        "label": "run_debug",
        "kind": 2,
        "importPath": "propagation",
        "description": "propagation",
        "peekOfCode": "def run_debug(Theta, X, y, lambda_reg):\n    np.set_printoptions(precision=5, suppress=True, floatmode='fixed')\n    A, Z, all_a_lists, all_z_lists = forward_propagation(Theta, X)\n    pred_y_list = [a_list[-1] for a_list in all_a_lists]\n    true_y_list = [y[i].reshape(-1, 1) for i in range(y.shape[0])]\n    J_list = []\n    for pred, true in zip(pred_y_list, true_y_list):\n        J = -(true.T @ np.log(pred) + (1 - true).T @ np.log(1 - pred))\n        J_list.append(J.item())\n    delta_list = []",
        "detail": "propagation",
        "documentation": {}
    }
]